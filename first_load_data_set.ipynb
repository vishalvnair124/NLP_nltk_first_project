{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414992ab-a192-44e7-8f5b-63fa76f4d9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08718cf2-9535-4451-981c-8fda8e2e8ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      1  ounce feather bowl hummingbird opec moment ala...\n",
      "1      1  wulvob get your medircations online qnb ikud v...\n",
      "2      0   computer connection from cnn com wednesday es...\n",
      "3      1  university degree obtain a prosperous future m...\n",
      "4      0  thanks for all your answers guys i know i shou...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the CSV file\n",
    "data = pd.read_csv(\"combined_data.csv\")\n",
    "\n",
    "# Displaying the first few rows of the data\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384ddf67-2f2a-4e31-946f-5fcdf85afc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label    0\n",
      "text     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ee66b4-790e-4188-b4df-da66f4f93fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'email', 'for', 'spam', 'classification', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example email for spam classification.\"\n",
    "\n",
    "# Tokenize the text using NLTK\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc65c71-856d-4e42-876b-924d2282d3eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Remove stopwords from the tokens\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m filtered_tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokens\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stopwords from the tokens\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d51638c-8724-429a-a43c-1bb3e1b17fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exampl', 'email', 'spam', 'classif', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize a stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stem each word\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0fe75f-27d7-4b72-a98a-18c5babf487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'email', 'spam', 'classification', '.']\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize a lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb3bb210-464e-4b5a-8ecf-e2c9c0c4821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  ounce feather bowl hummingbird opec moment ala...   \n",
      "1  wulvob get your medircations online qnb ikud v...   \n",
      "2   computer connection from cnn com wednesday es...   \n",
      "3  university degree obtain a prosperous future m...   \n",
      "4  thanks for all your answers guys i know i shou...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  ounce feather bowl hummingbird opec moment ala...  \n",
      "1  wulvob get medircations online qnb ikud viagra...  \n",
      "2  computer connection cnn com wednesday escapenu...  \n",
      "3  university degree obtain prosperous future mon...  \n",
      "4  thanks answer guy know checked rsync manual wo...  \n"
     ]
    }
   ],
   "source": [
    "# Assuming your dataset is loaded as a pandas DataFrame called 'data'\n",
    "texts = data['text'].values\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens (or you can use stemming)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the entire dataset\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Check the cleaned text\n",
    "print(data[['text', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5323137-9b45-4066-944a-fec6b940b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cleaned text for vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Now you can proceed with training your model using X and your labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d700f9da-21c1-4335-86c2-7f35e1646636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 66758\n",
      "Testing set size: 16690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels\n",
    "X = data['cleaned_text']\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace2ce44-491b-4224-8722-de75b6737249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF training set shape: (66758, 3000)\n",
      "TF-IDF test set shape: (16690, 3000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Check the shape of the TF-IDF matrices\n",
    "print(f\"TF-IDF training set shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF test set shape: {X_test_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "819dbdf7-f1d3-44b9-880a-17b36d4ee4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      7938\n",
      "           1       0.98      0.99      0.98      8752\n",
      "\n",
      "    accuracy                           0.98     16690\n",
      "   macro avg       0.98      0.98      0.98     16690\n",
      "weighted avg       0.98      0.98      0.98     16690\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "144a10b5-a56c-43d1-b126-700881851606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Email: hello , reminder weekly team check-in tomorrow 10 . see !\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Example input text\n",
    "new_email = \"Hello, just a reminder about our weekly team check-in tomorrow at 10 AM. See you then!\"\n",
    "\n",
    "# Preprocess the input text\n",
    "preprocessed_email = preprocess_text(new_email)\n",
    "print(f\"Preprocessed Email: {preprocessed_email}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "598e103a-7141-4a84-9f16-62805e19cb31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vector Shape: (1, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the preprocessed email\n",
    "new_email_tfidf = vectorizer.transform([preprocessed_email])\n",
    "\n",
    "# Check the shape of the TF-IDF matrix\n",
    "print(f\"TF-IDF Vector Shape: {new_email_tfidf.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9877e43e-74a1-4b8a-9431-7fa0d024001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email is classified as NOT SPAM.\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "prediction = model.predict(new_email_tfidf)\n",
    "\n",
    "# Interpret the prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"The email is classified as SPAM.\")\n",
    "else:\n",
    "    print(\"The email is classified as NOT SPAM.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f6ab9f7b-dbc8-4f9c-9f62-a84d825be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Train your model and vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "X = data['cleaned_text']\n",
    "y = data['label']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize and fit the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the model and vectorizer\n",
    "with open('model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e3d719-7de2-4b11-907e-5a944c363c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
